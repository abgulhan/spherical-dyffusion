# @package _global_

# to execute this experiment run:
# python run.py experiment=soma

defaults:
  - override /scheduler@module.scheduler: cosine_annealing.yaml
  - override /datamodule: soma_datamodule.yaml
  - override /model: fno3d_interpolation.yaml
  - _self_

name: "SOMA"
module:
  use_ema: True
  ema_decay: 0.9999
  monitor: "val/avg/crps" # if using module.num_predictions > 1, automatically changes to "val/avg/crps"
  num_predictions: 2 # Ensemble size. Effective evaluation batch size is datamodule.eval_batch_size * module.num_predictions
  optimizer:
    name: adamw  # or set to "adamw" if not using apex.FusedAdam
    lr: 1e-4  # Reduced learning rate for stability
    weight_decay: 1e-4  # Reduced weight decay
  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: 60

trainer:
  max_epochs: 60
  gradient_clip_val: 1.0  # Increased gradient clipping for stability
  deterministic: True
  precision: 16
  fast_dev_run: False  # Can be overridden from command line

#datamodule:
  #batch_size: 32    # global/effective batch size, needs to be divisible by the world size (number of GPUs)
  #batch_size_per_gpu: 1  # Optionally, set the maximum batch size per GPU. Gradient accumulation will be used to achieve the global desired batch size.
  #eval_batch_size: 4  # Increased to support multi-GPU evaluation
#  max_val_samples: 32

logger:
  wandb:
    entity: "abg"    # Set to your Weights & Biases username
    project: "Spherical-DYffusion"  # Use original project name with proper capitalization
    tags: ["soma", "fno3d", "ocean", "3d"]
    group: "SOMA"  # Use group to organize SOMA experiments
