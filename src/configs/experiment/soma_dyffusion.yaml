# @package _global_

# to execute this experiment run:
# python run.py experiment=soma_dyffusion

# Important:
# 1. This experiment requires the interpolator to be trained first.
#   The interpolator run id should be set in the `diffusion.interpolator_run_id` field below.
# 2. After training, try running inference with each of the checkpoints saved. Sometimes earlier checkpoints perform better.

defaults:
  - soma.yaml  # base config for SOMA dataset (edit if needed)
  - override /module: forecasting_multi_horizon_dyffusion.yaml
  - override /diffusion: dyffusion.yaml
  - _self_

name: "SOMA-DY-${datamodule.horizon}h"

datamodule:
  data_path: /global/homes/a/abgulhan/WORKING/ml_converted/data-gm-year7-22-biweekly.hdf5
  batch_size: 8  # Optimized for 40GB A100s - can handle larger batches
  batch_size_per_gpu: 2  # Each GPU processes 2 samples per batch
  eval_batch_size: 4
  num_workers: 8  # Increased for better data loading
  horizon: 24
  time_interval: 14  # days between samples, not used
  prediction_horizon: 504        # validation loader 1
  prediction_horizon_long: 1464  # validation loader 2 (every inference_val_every_n_epochs epochs)

module:
  enable_inference_dropout: False
  inference_val_every_n_epochs: 10  # go for prediction_horizon_long steps every 10 epochs

diffusion:
  interpolator_run_id: ???  # Read off the Weights & Biases ID from the interpolator run
  # Which checkpoint to use from the interpolator run:
  interpolator_wandb_ckpt_filename: "last.ckpt"  # "best-val_avg_crps.ckpt"

callbacks:
  model_checkpoint_time_mean_rmse_temperature:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/time_mean/rmse/timeDaily_avg_activeTracers_temperature"   # name of the logged metric which determines when model is improving
    mode: "min"         # "max" means higher metric value is better, can be also "min"
    save_top_k: 1               # save k best models (determined by above metric)
    save_last: False            # already saved in normal model checkpoint
    verbose: ${verbose}
    dirpath: ${ckpt_dir}
    filename: "${name}_${name_suffix}_epoch{epoch:03d}_seed${seed}"
    auto_insert_metric_name: False
  model_checkpoint_time_mean_rmse_salinity:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/time_mean/rmse/timeDaily_avg_activeTracers_salinity"   # name of the logged metric which determines when model is improving
    mode: "min"         # "max" means higher metric value is better, can be also "min"
    save_top_k: 1               # save k best models (determined by above metric)
    save_last: False            # already saved in normal model checkpoint
    verbose: ${verbose}
    dirpath: ${ckpt_dir}
    filename: "${name}_${name_suffix}_epoch{epoch:03d}_seed${seed}"
    auto_insert_metric_name: False

logger:
  wandb:
    tags: ["soma", "dyffusion"]
